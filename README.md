# Local Agent: Fast AI Backend with Docker Model Runner

A flexible, extensible AI agent backend built with NestJSâ€”designed for running local, open-source LLMs (Llama, Gemma, Qwen, DeepSeek, etc.) via Docker Model Runner. Real-time streaming, Redis messaging, web search, and Postgres memory out of the box. No cloud APIs required!

---

## ğŸš€ Quick Start

1. **Clone the repository**
   ```bash
   git clone <your-repo-url>
   cd <your-repo-folder>
   ```
2. **Copy and edit environment variables**
   ```bash
   cp .env.example .env
   # Edit .env and fill in your model and service config
   ```
3. **Start required services (Redis, PostgreSQL, Local LLM) with Docker Compose**
   ```bash
   docker compose up -d
   ```
   - PostgreSQL: `localhost:5433`
   - Redis: `localhost:6379`
   - Local LLM runner: `localhost:12434` ([Model Runner guide](https://blog.agentailor.com/posts/docker-model-runner-gemma))
4. **Install dependencies**
   ```bash
   pnpm install
   ```
5. **Start the development server**
   ```bash
   pnpm run start:dev
   ```

---

## ğŸ› ï¸ Environment Variables

See `.env.example` for all options. Key variables:

- `MODEL_BASE_URL` â€” e.g. `http://localhost:12434/engines/llama.cpp/v1`
- `MODEL_NAME` â€” e.g. `ai/gemma3:latest`, `llama-3`, `qwen`, `deepseek`
- `TAVILY_API_KEY` â€” for web search ([Get your key](https://www.tavily.com/))
- `REDIS_HOST`, `REDIS_PORT`, etc.
- `POSTGRES_*` â€” for memory

---

## âœ¨ Features

- ğŸ¤– Local, open-source LLMs (Llama, Gemma, Qwen, DeepSeek, etc.)
- ğŸŒŠ Real-time streaming responses
- ğŸ’¾ Conversation history with Postgres memory
- ğŸŒ Web search integration (Tavily)
- ğŸ§µ Custom ThreadService for conversations
- ğŸ“¡ Redis pub/sub for real-time messaging
- ğŸ¯ Clean, maintainable architecture

---

## ğŸ§© Model Setup (Docker Model Runner)

- This project is designed for local LLMs only, using [Docker Model Runner](https://blog.agentailor.com/posts/docker-model-runner-gemma).
- Supported models: Llama, Gemma, Qwen, DeepSeek, and other open-source models.
- Set `MODEL_BASE_URL` and `MODEL_NAME` in your `.env`.
- Start the `ai_runner` service with Docker Compose.
- For other providers, see [Agent Initializr](https://initializr.agentailor.com/).

---

## ğŸ”Œ Web Search (Tavily)
- Set `TAVILY_API_KEY` in `.env`
- Example usage in code:
  ```typescript
  AgentFactory.createAgent(
    ModelProvider.LOCAL,
    [new TavilySearch({ maxResults: 5, topic: 'general' })],
    postgresCheckpointer,
  );
  ```

---

## ğŸ—„ï¸ Project Structure

```
src/
â”œâ”€â”€ agent/       # AI agent implementation
â”œâ”€â”€ api/         # HTTP endpoints and DTOs
â””â”€â”€ messaging/   # Redis messaging service
```

---

## ğŸ›£ï¸ API Endpoints

- `POST /api/agent/chat` â€” Send a message to the agent
- `GET /api/agent/stream` â€” Stream agent responses (SSE)
- `GET /api/agent/history/:threadId` â€” Get conversation history
- `GET /api/agent/threads` â€” List all threads

---

## ğŸ’¬ Chat UI

For a ready-to-use frontend, use [agentailor-chat-ui](https://github.com/IBJunior/agentailor-chat-ui), which is fully compatible with this backend.

---

## ğŸ“ Required: Postgres Saver Checkpointer

This project uses Postgres for memory. You must initialize the checkpointer before chatting:
```typescript
// In agentService
async stream(message: SseMessageDto): Promise<Observable<SseMessage>> {
  const channel = `agent-stream:${message.threadId}`;
  // Run only once
  this.agent.initCheckpointer();
  // ...rest of code
}
```

---

## â„¹ï¸ Note

- This project is opinionated for local, open-source LLMs only.

---

## ğŸ“š Further Information

For more details and project resources, visit [Initializr](https://github.com/IBJunior/initializr).

---

## ğŸ“„ License

[MIT License](LICENSE)